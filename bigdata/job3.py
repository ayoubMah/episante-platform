from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, when, lit, concat, monotonically_increasing_id,
    current_timestamp, trim, upper, to_date
)
import uuid
import sys
import json
import os
from datetime import datetime, timezone

# â”€â”€ State Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STATE_FILE = "/home/spark/spark_jobs/state/job33_state.json"

def load_state() -> dict:
    if os.path.exists(STATE_FILE):
        with open(STATE_FILE, "r") as f:
            return json.load(f)
    return {
        "processed_years": [],
        "last_processed": None,
        "status": "done"
    }

def save_state(processed_years: list, status: str = "done"):
    os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)
    state = {
        "processed_years": sorted(processed_years),
        "last_processed": datetime.now(timezone.utc).isoformat(),
        "status": status
    }
    with open(STATE_FILE, "w") as f:
        json.dump(state, f, indent=2)
    print(f"   â†’ Ã‰tat sauvegardÃ© : {state}")

# â”€â”€ Config Spark OPTIMISÃ‰E â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
spark = SparkSession.builder \
    .appName("SPARCS_Silver_Incremental") \
    .config("spark.driver.memory", "6g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.sql.shuffle.partitions", "4") \
    .config("spark.default.parallelism", "4") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "64MB") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.executor.heartbeatInterval", "60s") \
    .config("spark.network.timeout", "300s") \
    .getOrCreate()

HDFS_BRONZE_PATH = "hdfs://192.168.1.30:9000/bronze/sparcs_parquet"
MONGO_URI        = "mongodb://192.168.1.40:27017"
MONGO_DB         = "sirius"
MONGO_COLLECTION = "sparcs_silver"
BATCH_ID         = str(uuid.uuid4())[:8]

def list_hdfs_years() -> set:
    """Liste les annÃ©es HDFS via metadata uniquement (sans lire les donnÃ©es)."""
    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(
        spark._jsc.hadoopConfiguration()
    )
    hdfs_path = spark._jvm.org.apache.hadoop.fs.Path(HDFS_BRONZE_PATH)
    years = set()
    try:
        for status in fs.listStatus(hdfs_path):
            name = status.getPath().getName()
            if name.startswith("Discharge_Year="):
                try:
                    years.add(int(name.split("=")[1]))
                except ValueError:
                    pass
    except Exception as e:
        print(f"    Erreur lecture HDFS metadata : {e}")
    return years
def main():
    print("===== Job 3 INCRÃ‰MENTAL - Bronze â†’ Silver =====")

    processed_years = set()
    try:
        # â”€â”€ 1. DÃ‰TECTION INCRÃ‰MENTALE via STATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\n DÃ©tection incrÃ©mentale...")

        state = load_state()
        processed_years = set(state["processed_years"])
        last_status = state["status"]

        print(f"   â†’ Statut prÃ©cÃ©dent       : {last_status}")
        print(f"   â†’ AnnÃ©es dÃ©jÃ  traitÃ©es   : {sorted(processed_years)}")

        if last_status == "running":
            print("   Job prÃ©cÃ©dent interrompu â†’ reprise forcÃ©e")
            processed_years = set()

        hdfs_years = list_hdfs_years()
        print(f"   â†’ HDFS Bronze disponible : {sorted(hdfs_years)}")

        remaining_years = sorted(hdfs_years - processed_years)
        print(f"   â†’ AnnÃ©es restantes       : {remaining_years}")

        if not remaining_years:
            print(" Aucune nouvelle donnÃ©e Ã  traiter.")
            spark.stop()
            sys.exit(0)
            # Traiter uniquement l'annÃ©e la plus ancienne non traitÃ©e
        year_to_process = remaining_years[0]
        print(f"   â†’  Traitement de l'annÃ©e : {year_to_process}")

        # Marquer "running" avant de commencer
        save_state(list(processed_years), status="running")

        # â”€â”€ 2. LECTURE HDFS â€” une seule annÃ©e â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print(f"\n Lecture HDFS â†’ Discharge_Year={year_to_process}...")
        path = f"{HDFS_BRONZE_PATH}/Discharge_Year={year_to_process}"
        df_bronze = spark.read.parquet(path).coalesce(4)

        # RecrÃ©er Discharge_Year (absent quand on lit un seul dossier partitionnÃ©)
        df_bronze = df_bronze.withColumn("Discharge_Year", lit(year_to_process).cast("int"))

        df_bronze.cache()
        total_bronze = df_bronze.count()
        print(f"   â†’ {total_bronze:,} lignes chargÃ©es")

        # â”€â”€ 3. NETTOYAGE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\n Nettoyage")

        # Discharge_Year retirÃ© de COLS_REQUIRED (recrÃ©Ã©e via lit())
        COLS_REQUIRED = [
            "Health_Service_Area",
            "Hospital_County",
            "Age_Group",
            "Gender",
            "APR_Severity_of_Illness_Description",
            "CCS_Diagnosis_Description"
        ]

        df_clean = df_bronze \
            .dropDuplicates() \
            .dropna(subset=COLS_REQUIRED) \
            .cache()
        clean_count = df_clean.count()

        df_bronze.unpersist()
        print(f"   â†’ {clean_count:,} lignes aprÃ¨s nettoyage")

        # â”€â”€ 4. TRANSFORMATIONS SILVER (selon PDF) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\n Transformations Silver...")

        df_silver = df_clean \
            \
            .withColumn("appointment_region",  trim(col("Health_Service_Area"))) \
            .withColumn("appointment_county",  trim(col("Hospital_County"))) \
            \
            .withColumn("patient_age_group",   trim(col("Age_Group"))) \
            .withColumn("patient_gender",      trim(upper(col("Gender")))) \
            \
            .withColumn("calendar_year",       col("Discharge_Year").cast("int")) \
            .withColumn("calendar_month",      lit(6).cast("int")) \
            .withColumn("calendar_year_month", concat(
                col("Discharge_Year").cast("string"), lit("-06")
            )) \
            \
            .withColumn("severity_label",      trim(col("APR_Severity_of_Illness_Description"))) \
            .withColumn("complexity_level",
                when(col("APR_Severity_of_Illness_Description") == "Minor",    lit(1))
                .when(col("APR_Severity_of_Illness_Description") == "Moderate", lit(2))
                .when(col("APR_Severity_of_Illness_Description") == "Major",    lit(3))
                .when(col("APR_Severity_of_Illness_Description") == "Extreme",  lit(4))
                .otherwise(lit(1))) \
            .withColumn("appointment_duration_minutes",
                when(col("complexity_level") == 1, lit(15))
                .when(col("complexity_level") == 2, lit(30))
                .when(col("complexity_level") == 3, lit(45))
                .when(col("complexity_level") == 4, lit(60))
                .otherwise(lit(15))) \
            \
            .withColumn("reason_label",                 trim(col("CCS_Diagnosis_Description"))) \
            \
            .withColumn("appointment_id",               monotonically_increasing_id()) \
            .withColumn("metadata_ingestion_timestamp", current_timestamp()) \
            .withColumn("metadata_batch_id",            lit(BATCH_ID))
        # Colonnes Silver finales â€” Drop tout le reste (Race, Ethnicity, Zip, admin...)
        SILVER_COLS = [
            "appointment_id",                   # appointment.id
            "appointment_region",               # appointment.region
            "appointment_county",               # appointment.county
            "patient_age_group",                # patient.age_group
            "patient_gender",                   # patient.gender
            "calendar_year",                    # calendar.year
            "calendar_month",                   # calendar.month
            "calendar_year_month",              # calendar.year_month (YYYY-MM)
            "severity_label",                   # severity.label
            "complexity_level",                 # complexity.level (1..4)
            "appointment_duration_minutes",     # appointment.duration_minutes
            "reason_label",                     # reason.label
            "metadata_ingestion_timestamp",     # metadata.ingestion_timestamp
            "metadata_batch_id"                 # metadata.batch_id
        ]
        df_silver_final = df_silver.select(SILVER_COLS)

        # â”€â”€ 5. Ã‰CRITURE MONGODB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print(f"\n Ã‰CRITURE MongoDB â†’ {MONGO_DB}.{MONGO_COLLECTION}")
        df_silver_final.write \
            .format("mongodb") \
            .mode("append") \
            .option("connection.uri", MONGO_URI) \
            .option("database", MONGO_DB) \
            .option("collection", MONGO_COLLECTION) \
            .save()
        print(" Ã‰criture terminÃ©e !")

        # â”€â”€ 6. VÃ‰RIFICATION FINALE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\nğŸ” VÃ©rification MongoDB...")
        df_check = spark.read \
            .format("mongodb") \
            .option("connection.uri", MONGO_URI) \
            .option("database", MONGO_DB) \
            .option("collection", MONGO_COLLECTION) \
            .load()
        df_check.groupBy("calendar_year").count().orderBy("calendar_year").show()

        # â”€â”€ 7. SAUVEGARDER L'Ã‰TAT FINAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        all_done = sorted(processed_years | {year_to_process})
        save_state(all_done, status="done")

        df_clean.unpersist()

        print(f"\n Job 3 INCRÃ‰MENTAL terminÃ© !")
        print(f"   AnnÃ©e traitÃ©e          : {year_to_process}")
        print(f"   Total annÃ©es traitÃ©es  : {all_done}")
        print(f"   AnnÃ©es restantes       : {sorted(hdfs_years - set(all_done))}")

    except Exception as e:
        save_state(list(processed_years), status="error")
        print(f"\n Erreur Job 3 : {e}")
        import traceback
        traceback.print_exc()
        spark.stop()
        sys.exit(1)

    spark.stop()

if __name__ == "__main__":
    main()